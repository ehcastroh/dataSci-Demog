{"metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "anaconda-cloud": {}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python", "version": "3.6.1", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}}, "nbformat": 4, "nbformat_minor": 1, "cells": [{"metadata": {}, "source": ["Demography 88<br>\n", "Fall 2017<br>\n", "Carl Mason (cmason@berkeley.edu)<br>\n", "# Lab 8: The Mariel Boatlift: a \"*Natural Experiment*\"\n", "\n", "This week and next, we will explore the labor market effects of the Mariel Boatlift.  David Card (world famous economist who happens to teach at Berkeley) saw the boatlift as a \"natural experiment\" -- in which the supply of labor in Miami was increased suddenly, unexpectedly and in some important ways: randomly.  In his famous 1990 paper,  \"The Impact of the Mariel Boatlift on the Miami Labor Market\", Card sets out to measure the impact of this labor market change on wages and employment status of pre-boatlift residents of Miami.\n", "\n", "This week and next, we will first replicate and then modestly extend Card's paper.  Of course, this would be an excellent time to have **already** read the paper -- which is available on jstor as well as at: [http://davidcard.berkeley.edu/papers/mariel-impact.pdf]\n", "\n", "The key substantive issue is whether and to what extent an increase in the labor supply leads to a reduction in wages of those previously resident.  This is precisely the question that we addressed in our discussion of Clemen's trillion dollar bills and the simple neoclassical model of immigration depicted in this totally viral app: http://shiny.demog.berkeley.edu/carlm/EconImmig0/,\n", "\n", "The key technical challenge is to bootstrap a statistic that is  more complicated than a mean -- and the incorporation of sample weights. In many ways this project is structurally very similar to the Wage Impact lab from last week, except that it is a bit more complicated.\n", "\n", "What we are going to ultimately do is called a \"difference in differences\" estimate.  We'll get into details of that next week, but what you need to keep in mind this week is that we're going to compute the *difference* between the mean earnings of a well defined group of workers in Miami and a similar group of workers in a \"control\" group of similar cities. We're going to do that difference computation at a time *before* the Mariel boatlift and again a few years *after* the boatlift.  The we'll compare the two differences.  If the difference got bigger(or smaller)  we can attribute that change to the Mariel boatlift.  Essentially, we will assume that anything important (but unrelated to the Mariel boatlift) that happened during that interval -- had an equal effect on both Miami and the control cities.  As usual, we are oversimplifying a bit -- but the gist of it is that the \"difference in differences\" framework, provides a way of \"controlling\" for all sorts of unobserved things.  \n", "\n", "It is probably worth re-reading the above paragraph, because if you get what it's saying,  this lab is going to make a lot more sense.\n", "\n", "\n", "The steps involved are:\n", "1. Recreate tables 1, 3 and 4 of Card's famous paper.\n", "1. Draw some graphs to illustrate the quantities (differences) in question.\n", "2. Compare our findings with Card's regression based  outcomes.\n", "3. Use bootstrap to quantify our uncertainty about the effects on the labor market disaggregated by education and occupation -- tests which are not part of the original paper.\n", "\n", "We will accomplish steps 1&2 this week and 3&4 next week.\n", "\n", "The data that we will use is of course the same as that used by Professor Card: the \"Monthly Outgoing Rotation Groups\" from the Current Population Survey (CPS). The CPS is a venerable survey which has been consistently conducted since unemployment was invented in the 1930s.  The survey is done every month by the Census Bureau for the Bureau of Labor Statistics and its purpose is to measure unemployment.  To do so  it asks a series of questions which come down to: \"Do you have a job\" and if not \"Are you looking for one\".  The latter determines whether or not the respondent is \"in the labor force\" and the former whether or not the person is employed. \n", "\n", "Of course, because surveys of this sort are quite expensive, the CPS does not merely ask the Employed?/Looking? questions but rather administers an impressive battery of questions on a wide range of topics including things closely related to employment such as  education and occupation, but also on topics of broader  interest to academics and policy makers-- such as computer and internet use; health insurance; tobacco use; veteran status; migration and civic engagement. Most of these questions are contained in what are known as monthly \"supplements\" to the \"base survey\" (which is concerned with employment).  Best of all, lots these data are available through IPUMS http://ipums.org. The data for this lab, however, were downloaded originally from the NBER site which makes the data available in a form much closer to what Professor Card used back in the Twentieth Century.\n", "http://http://www.nber.org/data/morg.html.\n", "\n", "The \"outgoing rotation groups\" (ORG) refers to the way the survey is conducted: each month a new group of respondents is selected. Each group is then polled for four consecutive months; then left alone for next eight months; and then polled again for four months. The income and employment questions in which we are interested, are asked *only* in the fourth and eighth interviews. This all means that:\n", "\n", "1. Each respondent (household) is asked these questions two times during their 16 months of survey participation.\n", "2. Each month only one eighth of currently empaneled CPS respondents are asked these questions.\n"], "cell_type": "markdown"}, {"source": ["# Run this cell to import the stuff we'll need\n", "import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt \n", "plt.style.use('fivethirtyeight')\n", "\n", "%matplotlib inline\n", "from datascience import Table\n", "from datascience.predicates import are\n", "from IPython.display import HTML, IFrame, display\n", "datasite=\"http://courses.demog.berkeley.edu/mason88/data/\"\n", "quizsite=\"http://courses.demog.berkeley.edu/mason88/cgi-bin/quiz.py\"\n", "  \n", "def cquiz(qno) : \n", "    import IPython, requests \n", "    try:\n", "        sid\n", "    except NameError: \n", "        print(\"HEY! did you enter your sid way up at the top of this notebook?\")\n", "    Linkit='{0}?qno={1}&sid={2}'.format(quizsite,qno,sid)\n", "    #print(Linkit)\n", "    html = requests.get(Linkit)\n", "    #display(IFrame(Linkit, 1000, 300))\n", "    display(IFrame(Linkit, 1000, 400))\n", "\n", "\n", "    \n", "######################\n", "# Here it is ... the obvious place to put your student id\n", "sid=\"\"\n", "######################\n", "if sid == \"\" :\n", "    print(\"HEY! didn't I tell you to put your sid in the obvious place\")\n", " \n"], "metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["   \n", "###\n", "# put your student id in the obvious place below - or you will find yourself typing it quite frequently\n", "###\n", "sid=\"\"\n", "\n", "if sid == \"\" :\n", "    print(\"HEY! didn't I tell you to put your sid in the obvious place\")"], "metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["### #### #### ####\n", "###  SELECTING  LAB  PARTNERS\n", "### ### #### ####\n", "\n", "N=24\n", "numbers=np.arange(N)\n", "print(np.mod(N,2))\n", "if (not np.mod(N,2) == 0) :\n", "    numbers=np.append(numbers,\"lucky\")\n", "    N+=1\n", "numbersTab=Table().with_column('n',numbers)\n", "randomized=numbersTab.sample(k=N,with_replacement=False)\n", "selection=randomized['n']\n", "selection.shape = (2,int(N/2))\n", "Table().with_columns('zero',selection[0],'one',selection[1]).show()"], "metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["## With whom are you working today?\n", "cquiz('mariel-01partners')"], "metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["# Read the data -- \"morg\" stands of \"merged outgoing rotation groups\" which are a subset of the data collected in \n", "# Current Population Survey\n", "\n", "morg=Table.read_table(datasite+\"morgClean.csv\")\n", "\n", "# and add the variable lnWage for log of hourly wage\n", "morg=morg.with_column('lnWage',np.log(morg['Earnhr']))\n", "\n", "print(np.min(morg['age']))\n", "print(np.max(morg['age']))\n", "morg"], "metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["## Recreating Table 1\n", "\n", "Table 1 in Card's paper,  gives some descriptive statistics for Miami in 1979 before the Mariel Boatlift happens.\n", "\n", "Recreating it will give us a feel for both the structure of the data and the situation in Miami in 1979.\n", "\n", "Because the CPS is responsible for producing an unemployment statistic for each and every state, county and SMSA-- by race, ethnicity and other demographic characteristics, a *simple random sample* turns out **not** to be the best way to do it.  Collecting these data are expensive, and in order to be assured of having enough observations in **each** geographical/demographic category, a simple random sample would have to be very large (expensive).  The way this is avoided is with a \"stratified\" random sample, which is a type of \"probability sample\" (as described: https://www.inferentialthinking.com/chapters/08/5/sampling.html) .  To oversimplify a bit,  a stratified random sample contains a certain number of observations chosen at random from *each*  geographical area of interest. Then weights are calculated to adjust statistics that are computed from those data.  In order to calculate theses weights it is necessary to know each observation's probability of selection.  In the simplest case, this is just a the number of observations collected from the area divided by the number of individuals in the area who could have been selected. In the case of the CPS, the sample is stratified in many more ways besides geography, so even within a region and year the weights vary across individuals. \n", "\n", "The way to use weights, in the present (typical) case, is to think of them as the number of people in the universe who are *represented* by each individual in the sample.  Thus the number of people in area A =$\\sum_{i\\in{A}}{w_i}$ where $w_i$ is the sample weight of individual i and $i\\in{A}$ indicates that individual i lives in area A.\n", "\n", "Similarly, the weighted average of some characteristic, c, of people who live in area A would be given by: \n", "$$\\frac{\\sum_{i\\in{A}}{(c_i*w_i)}}{\\sum_{i\\in{A}}{w_i}}$$\n", "\n", "The above formulae are what David Card used to compute the figures in his Table 1.  It should also look pretty familiar--it is the same formula that we used in the Wage Impact lab."], "cell_type": "markdown"}, {"source": ["## So turning to Table 1\n", "## Call this cell \"code for panel 1\"\n", "## Here is some code to generate the top panel\n", "\n", "## select observations from Miami in 1979\n", "\n", "miami79=morg.where('SMSA',\"Miami\").where('year',1979)\n", "# create columns of c_i * w_i  characteristic * weight\n", "miami79.append_column('EducWT',miami79['Educ']*miami79['Weight3'])\n", "miami79.append_column('LabfWT',miami79['Labf']*miami79['Weight3'])\n", "# sum the c_i*w_i and the w_i  to get numerator and denominators from equation above\n", "tab1_1=miami79.select(['EthRace','Weight3','EducWT','LabfWT']).\\\n", "    groups('EthRace',collect=np.nansum)\n", "\n", "# relabel to remind ourselves that the sum of the wieghts is the number of observations, N\n", "tab1_1.relabel('Weight3 nansum','N')\n", "# weighted means per the equation above\n", "tab1_1.append_column('EducWTM',tab1_1['EducWT nansum']/tab1_1['N'])\n", "tab1_1.append_column('LabfWTM',tab1_1['LabfWT nansum']/tab1_1['N'])\n", "tab1_1"], "metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["# What do we think of these numbers ?\n", "\n", "Profound discussion question: How close is close enough ?\n"], "cell_type": "markdown"}, {"metadata": {}, "source": ["## A note on weights\n", "\n", "Constructing weights for survey samples is somewhat of a dark art.  While the principles are well understood, the execution invariably requires some judgement and compromise--mainly because the weighting always requires information external to the survey sample itself.  The CPS, for example, requires census data in order to know how many households exist in a particular county, as well as the demographic, racial and ethnic makeup of the region.  And if that were not enough, the Census is taken every 10 years, so  in years like 1979, the available census is quite out of date -- more judgement and external data are required in order to come up with estimates of 1979 population totals.\n", "\n", "To get a flavor of what's involved, I half heartedly recommend taking a quick look at: https://www.census.gov/prod/2006pubs/tp-66.pdf.\n", "\n", "The complexity/fragility of the weighting process is part of the reason that the CPS contain TWO weighting variables:  Weight3, that we have used thus far is one,  but there is also a column called 'Earnwt' which is a very slightly different weight that is supposed to be used for all earning related variables.  \n", "\n", "\n"], "cell_type": "markdown"}, {"source": ["# scatter plot of two weighting variables\n", "morg.scatter('Weight3','Earnwt',colors='year')"], "metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["For your edification here are descriptions of the two weights from the NBER documentation:\n", "\n", "weight:\n", ">This is the Final Weight. The sum of the Final Weights in each monthly survey is the US non-institutional population. The CD-ROM excludes persons under 16 years of age. The outgoing rotation group includes one-fourth of that population. So one single month MORG file is one-fourth the population 16 years of age and over,and a year of MORG would sum to 3 times that population. Zero weights appear in some years, for records of unknown function. The implied two or four (1994 on) decimals on the tapes are explicit here. 1990-census-based weight for 2000-2002 are is\n", "available as weightp.\n", "\n", "earnwt:\n", ">Earnings weight for all races. Used for tabulating earnings related items. Since the CD-ROM includes all persons asked earning questions, this sums to the total population each month and 12 times the population for each MORG file. This is not precisely 4 times the weight, presumably because the Census has external knowledge of the size and composition of the labor force. The implied decimals on the tapes are explicit here. A BLS letter suggests that this weight is preferred for all purposes. 1990-census-based earnwt for 2000-2002 is available as earnwtp. \n", "\n", "\n", "Fortunately, these details don't matter very much. I have experimented with both weights and (and with several other possible data adjustments and determined 2 things:\n", "\n", "1. The weights matter very little\n", "2. It is not possible to *precisely* reproduce Card's numbers despite having access to his computer code.\n", "\n", "\n"], "cell_type": "markdown"}, {"metadata": {}, "source": ["## Table 1  panel 2   (Code Required)\n", "\n", "** Copy then modify the \"code for panel 1\" in order to produce panel 2 of Table 1**\n", "\n", "Some notes:\n", "\n", "1. 'Labf' is a True/False variable indicating whether the individual was in the labor force\n", "2. 'age' is age in years those under 16 are not in the dataset\n", "3. 'Educ' gives the number of years of school completed \n", "4. Your instructor believes that the weighted average number of years of education of non hispanic Blacks in Miami in 1979 was 11.7145.\n", "4. When you have completed panel 2 of table 1 - click on the quiz link below to answer an easy question and get credit for your excellent work."], "cell_type": "markdown"}, {"source": ["## Copy \"code for panel 1\" into this cell and modify it to produce panel 2 in Card's Table 1\n", "# See instructions in cell above\n", "# \n", "\n", "\n", "\n", "\n", "\n", "tab1_2.show()"], "metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": [], "metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["cquiz('mariel-01')"], "metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["\n", "# Here's panel 3 of Table 1\n", "\n", "# These numbers might be useful next week, we won't do anything with them this week, however.\n", "tab1_3=miami79.where('Unemp',False).select(['EthRace','Occupation','Weight3']).\\\n", "    groups(['EthRace','Occupation'],collect=np.nansum)\n", "tab1_3sums=miami79.where('Unemp',False).select(['EthRace','Weight3']).\\\n", "    groups(['EthRace'],collect=np.nansum)\n", "# Here's the clever bit -- we need to divide by N in tab\n", "tab1_3=tab1_3.join('EthRace',tab1_3sums,'EthRace')\n", "tab1_3=tab1_3.with_column('Proportion',tab1_3['Weight3 nansum']/tab1_3['Weight3 nansum_2'])\n", "print('Table 1 panel 3: Miamai 1979 occumpational distribution')\n", "tab1_3.pivot('EthRace','Occupation',values='Proportion',collect=np.array)\n"], "metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["# Let's remind ourselves of how the morg dataset works\n", "cquiz('mariel-02')"], "metadata": {"scrolled": true}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["## Recreating Table 3  log(hourly wage)\n", "\n", "Let's turn now to Table 3. Below is some code to generate an UNWEIGHTED version of the table.\n", "Your job is to modify the code to include weights.  \n", "\n", "#### It turns out that there are two sets of weights in the CPS.  For our purposes which we use does not matter much, but for reasons that I will bore you with later, we will use 'Earnwt' instead of 'weight3' from now on.\n"], "cell_type": "markdown"}, {"source": ["## The unweighted version of Table 3\n", "#for convenience a column that distinguishes Miami from the Control Cities\n", "morg.append_column('Miami',morg['SMSA'] =='Miami')\n", "# take the means of the lnWage for each important subgroup Miam/EthRace/year\n", "tab3_UW=morg.select(['Miami','EthRace','year','lnWage']).groups(['Miami','EthRace','year'],np.nanmean)\n", "# rename the columns for a more pleasing table\n", "tab3_UW=tab3_UW.relabel('lnWage nanmean','meanLnWage')\n", "print(\"UNWEIGHTED Table 3\")\n", "tab3_UW.show(10)\n", "\n", "## Now pivot table trick to reformat the table so that it is easier to compare to the one in the paper\n", "# \n", "# since there is ONE unique value for each category any \"collect\" function will work function\n", "\n", "print('Miami')\n", "tab3_UW.where('Miami',True).pivot('year','EthRace',values='meanLnWage',collect= np.array).show()\n", "print('Control')\n", "tab3_UW.where('Miami',False).pivot('year','EthRace',values='meanLnWage',collect=np.array).show()"], "metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["These results, without weighting are reasonably close but we should be able to get closer to what Card found if we use   weights"], "cell_type": "markdown"}, {"metadata": {}, "source": ["## NOW  produce the *weighted* version of Table 3  (Coding Required)\n", "\n", "The same principles apply in Table 3 as in Table 1 as far as weights are concerned.\n", "Below are a couple of lines to get you started. \n", "\n", "Notes:\n", "1. One gotcha here is that there are some observations which have a nonzero Earnwt but for which Earnhr is missing.  I am not sure how this happens -- possibly there are people whose earnings are all from capital ? whatever the reason -- they need to be dropped when computing the denominator, $\\sum{w_i}$.\n", "2. Your instructor believes that the weighted mean log hourly wage of of Non-Hispanic whites in Miami in 1983 was 1.779269 \n", "3. Use Earnwt for the weighting variable\n", "\n"], "cell_type": "markdown"}, {"source": ["# Table 3 with weights\n", "# create a new column containing Earnhr multiplied by the weight variable\n", "morg.append_column('lnWageWTD',np.log(morg['Earnhr'])*morg['Earnwt'])\n", "#  create new column indicating quality of Earnhr because there are observations \n", "#  with positive Earnwt and  nan for Earnhr\n", "#  The '~' operator is the 'not' operator; it inverts the boolean value of the expression that follows it\n", "#  ~np.isnan == is NOT nan\"\n", "morg.append_column('GoodWageDat',~np.isnan(morg['Earnhr']))\n", "\n", "## Code required here to complete the job of computing Table 3\n", "\n", "\n", "\n", "tab3= ???  #(could require more than one line of code)\n", "\n", "\n", "\n", "\n", "## pivot table trick to reformat tab3 to look like it does in the paper\n", "print('Miami')\n", "tab3.where('Miami',True).pivot('year','EthRace',values='meanLnWage',collect=np.array).show()\n", "print('Control')\n", "tab3.where('Miami',False).pivot('year','EthRace',values='meanLnWage',collect=np.array).show()"], "metadata": {"scrolled": true, "collapsed": true}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": [], "metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["## How much difference do the weights make?\n", "\n", "Discuss the histogram below.\n", "\n", "Consider the following back of the envelope way to assess whether the difference between the weighted and unweighted values in Table 3 are big enough to matter:  The key question is would it matter (much) to the results if we used the un-weighted values in Table 3 in place of the weighted values. Without doing any computation consider the things that we are likely to compare:\n", "1. Miami residents of a single EthRace across time.\n", "1. Miami residents of a single EthRace in a single year vs people of the same category in a control city.\n", "1. Residents of either place of a particular EthRace at a single year vs people of a different EthRace.\n", "\n", "Scan the weighted Table 3 that you just computed and determine for each of the three cases above, what \"large-ish\" difference would be.  \n", "\n", "Are each of these three \"large-ish\" values big or small compared to a large value in the histogram below:"], "cell_type": "markdown"}, {"source": ["## Comparing the values with and without weights\n", "\n", "# assuming your result from above is called tab3 and the mean log wage column is called 'meanLnWage'\n", "Table().with_column('weighted-unweighted',tab3['meanLnWage']-\\\n", "        tab3_UW['meanLnWage']).hist(bins=25)\n"], "metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["cquiz(\"mariel-031\")"], "metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["cquiz(\"mariel-04\")"], "metadata": {"scrolled": true}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["## Are the differences between your log wage estimates and Card's important? \n", "\n", "\n", "\n", "\n", "\n", "\n", "To inform your discussion... you may read in the values in Card's Table 3 by\n", "executing the following cell an then creating a histogram just like the one above?\n"], "cell_type": "markdown"}, {"source": ["## read data from Card's Table 3 which your instructor has thoughtfully typed for you\n", "cardT3=Table.read_table('http://courses.demog.berkeley.edu/mason88/data/cardTable3.csv')\n", "cardT3.append_column('Miami',cardT3['Miami'] =='yes')\n", "cardT3.relabel('meanLnWage','cardMeanLnWage')\n", "cardT3\n"], "metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["# Now to make the histogram comparing your results with Card's Table 3, we'll need to create a table wherein \n", "# one column holds Card's results and another holds your results. This should be a \"simple\" .join since cardT3\n", "# and tab3 are similarly structured with columns for EthRace,year and Miami.\n", "\n", "# It's tricker than it should be however, because .join only works on a single column -- and we would\n", "# like to join rows based on three columns.  A reasonable solution is to create\n", "## a variable, called 'rdx'  which is the concatination of EthRace,year and Miami\n", "\n", "# surprisingly THIS does NOT work:\n", "##cardT3.append_column(rdx,cardT3['EthRace']+cardT3['year'].astype(str))\n", "\n", "#this works:\n", "cardT3.append_column('rdx',np.char.add(\n", "    np.char.add(cardT3['EthRace'],cardT3['year'].astype(str)),cardT3['Miami'].astype(str)))\n", "\n", "tab3.append_column('rdx',np.char.add(\n", "    np.char.add(tab3['EthRace'],tab3['year'].astype(str)),tab3['Miami'].astype(str)))\n", "## and the join works\n", "t3diff=cardT3.select(['rdx','cardMeanLnWage']).join('rdx',tab3)\n", "\n", "# and the histogram\n", "Table().with_column('Card-You',t3diff['cardMeanLnWage']-t3diff['meanLnWage']).hist(bins=25)\n", "\n"], "metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["## In therory...\n", "\n", "there should be no difference between the numbers that you have computed and those that David Card reported.  And yet,  there *are* small differences.  Given that we are using the same data set and that your instructor had access to Card's computer program,  this is kind of sad. It could certainly be that your instructor has lead you astray (that's happened before) but, of course I don't think that's the case. \n", "\n", "What has happened is that time has passed and even though that \"monthly outgoing rotation groups\" data from 1979-85 *should* not have changed since Card published his famous paper, it appears that that may not be the case.  It appears that small changes to the data are made by both the BLS and NBER (who agregates the data).  The changes are not enough to affect results, but it does emphasize how difficult it is to reproduce results -- even using the same data!"], "cell_type": "markdown"}, {"source": ["cquiz('mariel-031')"], "metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["# Summarize what we know so far with a few graphs\n", "\n", "### The key question in this lab and the next is: to what extent, if any, were workers who lived in Miami in 1979 made worse (or better) off *as a result of* the influx of Cuban immigrants in 1980. Let's draw some graphs that compare wage trajectories of subgroups of Miami workers with similar people in the control group cities.\n", "\n", "We'll start by writing a function that will do the calculations that we have been doing all day -- taking weighted averages of a value grouped by other characteristcs such as year, Miami residence and EthRace. Then we'll use the output to draw some useful graphs. The function, which will be quite useful in next week's exercise,  is provided, the graphs will require some coding.\n"], "cell_type": "markdown"}, {"source": ["## The function is essentially a generalization of the code that you have already written\n", "## it is worth understanding how it works as we will use it again\n", "def wtdMean(data,depvar,gvars=['Miami','year'],wvar='Earnwt'):\n", "\n", "    \"\"\"\n", "    given a dataset (generally morg) a dependent variable column e.g. lnWage; a list of\n", "    variables by which to group the data, gvars, and a weight column (Earnwt)\n", "    returns a table with one row for each unique combination of gvars along  the corresponding\n", "    weighted mean of depvar\n", "    \"\"\"\n", "    # creat a list of columns of the input data that we need; we'll discard the rest\n", "    # note that we need to copy rather than asign here as we still need gvars\n", "    allvars=gvars.copy()\n", "    allvars.append(depvar)\n", "    allvars.append(wvar)\n", "    #get rid of columns we don't need\n", "    dset=data.select(allvars)\n", "    #get rid of rows that we don't want\n", "    dset=dset.where(~np.isnan(dset[wvar]))\n", "    dset=dset.where(~np.isnan(dset[depvar]))\n", "    # compute the numerator and denominator\n", "    dset.append_column(depvar+'WTD',dset[depvar]*dset[wvar])\n", "    result=dset.groups(gvars,collect=np.nansum)\n", "    # and do the division\n", "    result.append_column(depvar+\"WTDmean\",result[depvar+ 'WTD'' nansum']/result[wvar+' nansum'])\n", "    # return the result tossing out some intermediate calcultions\n", "    return(result.drop([depvar+\" nansum\",depvar+\"WTD\"+\" nansum\"]))"], "metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["## Question about the wtdMean() function\n", "cquiz('mariel-07')"], "metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["## An example using the wtdMean() function"], "cell_type": "markdown"}, {"source": ["# Example 0 - Using the wtdMean() funtion \n", "# mean wage of employed people in Miami vs control towns by year\n", "MiamixCntrl=wtdMean(morg.where('Unemp',False),'Earnhr')\n", "MiamixCntrl"], "metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["## Using the result of the wtdMean() function to contruct a graph"], "cell_type": "markdown"}, {"source": ["MiamixCntrl.scatter('year','EarnhrWTDmean',colors='Miami')"], "metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["wtdMean(morg.where('Labf',True),'Unemp',gvars=['Miami','year','EthRace'])\n", "\n"], "metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["## Interpret the graph above\n", "cquiz('mariel-06')"], "metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["## Another example using the wtdMean() funtion  + graphs"], "cell_type": "markdown"}, {"source": ["\n", "DepVar='Earnhr'\n", "temp=wtdMean(morg.where('Unemp',False).where('EthRace','NonHisp:black'),DepVar)\n", "temp.scatter('year',DepVar+'WTDmean',colors='Miami')"], "metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["DepVar='Earnhr'\n", "temp=wtdMean(morg.where('Unemp',False).where('EthRace','NonHisp:black').where('age',are.between_or_equal_to(20,30)),DepVar)\n", "temp.scatter('year',DepVar+'WTDmean',colors='Miami')"], "metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["# Congratulations you have completed this week's lab."], "cell_type": "markdown"}, {"source": ["cquiz('suro417')"], "metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["cquiz('mariel0-eval')"], "metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "cell_type": "code"}]}